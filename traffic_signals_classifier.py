# -*- coding: utf-8 -*-
"""Traffic_signals_classifier.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dVl_M6tmBuxFjTvJ5tD_6CdcatfmMTYc

#Import All libraries
"""

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
import pickle
from sklearn.utils import shuffle

"""#Getting dataset
(Note: Dataset is not present here, so the path is set according the past location used...)
"""

with open("./traffic-signs-data/train.p", mode='rb') as training_data:
    train = pickle.load(training_data)
with open("./traffic-signs-data/valid.p", mode='rb') as validation_data:
    valid = pickle.load(validatin = X[ion_data)
with open("./traffic-signs-data/test.p", mode='rb') as testing_data:
    test = pickle.load(testing_data)

X_train, Y_train = train["features"], train['labels']
X_test, Y_test = test["features"], test['labels']
X_valid, Y_valid = valid["features"], valid['labels']

"""Random image visualization..."""

i = np.random.randint(1, len(X_train))
plt.imshow(X_train[i])
Y_train[i]

# Define the dimensions of the plot grid 
W_grid = 5
L_grid = 5

# subplot return the figure object and axes object
# we can use the axes object to plot specific figures at various locations

fig, axes = plt.subplots(L_grid, W_grid, figsize = (20,20))
axes = axes.ravel() # flaten the 5 x 5 matrix into 25 array

n = len(X_train) # get the length of the training dataset
   
for i in range(25):
  
  # Select a random number from 0 to n_training
    ind = np.random.randint(0, n)
    axes[i].imshow(X_train[ind])
    axes[i].set_title(Y_train[ind], fontsize=20)
    axes[i].axis('off')
plt.subplots_adjust(hspace = 0.4)

"""Shuffling the train data to remove any sort of orderedness on the basis of labels that the model may learn (May be skipped)"""

X_train, Y_train = shuffle(X_train, Y_train)

"""#Conversion to gray scale images"""

X_train_gray = np.sum(X_train/3, axis=3, keepdims=True) #axis = 3 means calculate the sum accross axis 3, i.e, rgb...
X_valid_gray = np.sum(X_valid/3, axis=3, keepdims=True)
X_test_gray = np.sum(X_test/3, axis=3, keepdims=True)

print(X_train_gray.shape, X_valid_gray.shape, X_test_gray.shape)

"""Normalizing the gray scale images..."""

X_train_gray_norm = (X_train_gray - 128)/128
X_valid_gray_norm = (X_valid_gray - 128)/128
X_test_gray_norm = (X_test_gray - 128)/128

print(X_train[1], X_train_gray_norm[1].squeeze()

"""Visualizing the gray, normalized and normal images..."""

i = random.randint(1, len(X_train_gray))
plt.imshow(X_train_gray[i].squeeze(), cmap = 'gray')
plt.figure()
plt.imshow(X_train[i])
plt.figure()
plt.imshow(X_train_gray_norm[i].squeeze(), cmap = 'gray')

"""#Model"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Flatten, Dense, Conv2D, MaxPooling2D, Dropout

model = Sequential()
model.add(Conv2D(8,5, activation='relu', input_shape=(32,32,1)))
model.add(MaxPooling2D())
model.add(Dropout(0.2))
model.add(Conv2D(16,5, activation='relu'))
model.add(MaxPooling2D())
model.add(Dropout(0.2))
model.add(Flatten())

model.add(Dense(120, activation='relu'))
model.add(Dense(80, activation='relu'))
model.add(Dense(43, activation='softmax'))
model.summary()

model.compile(optimizer='Adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
hist = model.fit(X_train_gray_norm, Y_train, batch_size=500, epochs =5, verbose =1, 
                 validation_data=(X_valid_gray_norm, Y_valid))

hist.history.keys()

"""#Calculating the score..."""

score = model.evaluate(X_test_gray_norm, Y_test)
print('Test Accuracy: {}'.format(score[1]))

"""Plotting the loss and accuracy"""

accuracy = hist.history['accuracy']
val_accuracy = hist.history['val_accuracy']
loss = hist.history['loss']
val_loss = hist.history['val_loss']

plt.plot(range(len(accuracy)), accuracy, 'b', label='Train accuracy')
plt.plot(range(len(loss)), loss, 'r', label='Train Loss')

"""#Predicting the images..."""

predicted_classes = model.predict_classes(X_test_gray_norm)
y_true = Y_test

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_true, predicted_classes)
plt.figure(figsize = (25, 25))
sns.heatmap(cm, annot = True)

"""Visualizing the predcted images..."""

L = 5
W = 5

fig, axes = plt.subplots(L, W, figsize = (20, 15))
axes = axes.ravel()

for i in np.arange(0, L*W):
    axes[i].imshow(X_test[i])
    col = 'green' if predicted_classes[i]== y_true[i] else 'red'
    axes[i].set_title('Prediction = {}\n True = {}'.format(predicted_classes[i], y_true[i]), color=col)
    axes[i].axis('off')

plt.subplots_adjust(wspace = 1)

"""Names of according to the class names provided...

In this case study, we want to classify images of traffic signs using deep Convolutional Neural Networks (CNNs).

The dataset consists of 43 different classes of images.

Classes are as listed below:

0 = Speed limit (20km/h)
1 = Speed limit (30km/h)
2 = Speed limit (50km/h)
3 = Speed limit (60km/h)
4 = Speed limit (70km/h)
5 = Speed limit (80km/h)
6 = End of speed limit (80km/h)
7 = Speed limit (100km/h)
8 = Speed limit (120km/h)
9 = No passing
10 = No passing for vehicles over 3.5 metric tons
11 = Right-of-way at the next intersection
12 = Priority road
13 = Yield
14 = Stop
15 = No vehicles
16 = Vehicles over 3.5 metric tons prohibited
17 = No entry
18 = General caution
19 = Dangerous curve to the left
20 = Dangerous curve to the right
21 = Double curve
22 = Bumpy road
23 = Slippery road
24 = Road narrows on the right
25 = Road work
26 = Traffic signals
27 = Pedestrians
28 = Children crossing
29 = Bicycles crossing
30 = Beware of ice/snow
31 = Wild animals crossing
32 = End of all speed and passing limits
33 = Turn right ahead
34 = Turn left ahead
35 = Ahead only
36 = Go straight or right
37 = Go straight or left
38 = Keep right
39 = Keep left
40 = Roundabout mandatory
41 = End of no passing
42 = End of no passing by vehicles over 3.5 metric tons
"""